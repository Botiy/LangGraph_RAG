{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69dc94a2-fab4-4056-85bd-21573942f695",
   "metadata": {},
   "source": [
    "# Chatbot application using RAG\n",
    "> The goal of this notebook is to present a chatbot application, which is based on Retrieval Augmented Generation.\n",
    "\n",
    "Expectations:\n",
    "* The chatbot should demonstrate the basic elements of agentic behavior,\n",
    "autonomous decision making, autonomous decomposition of subtasks and\n",
    "execution.\n",
    "* The application must use the Retrieval-Augmented Generation (RAG)\n",
    "technique to integrate external knowledge sources.\n",
    "* The solution should include structured documentation of the design\n",
    "design decisions, architecture used and operational logic.\n",
    "\n",
    "## Steps\n",
    "1. Setting up the environment\n",
    "2. Loading and saving the data\n",
    "3. Splitting text into chunks\n",
    "4. Creating embeddings and building vector database\n",
    "5. Define tools and agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2289f4b3-af84-4e8b-b993-cfbdab83404c",
   "metadata": {},
   "source": [
    "### 1. Setting up the environment\n",
    "* Hugging Face -> models and datasets\n",
    "* LangChain -> chaining and retrieval \n",
    "* FAISS -> vector store\n",
    "* LangGraph -> agentic logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e46b4ba6-5043-49da-baa5-f60268b05772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Installing required packages\n",
    "!pip install -q langchain-community langgraph faiss-cpu huggingface_hub transformers sentence-transformers datasets tiktoken unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7673e9a0-aace-4b8d-b863-2167d22c859c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mwparserfromhell in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.6.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Installing parser\n",
    "!pip install mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "05fc20ed-8b5d-416f-b199-1063187a4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "# Core libraries\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from typing import List\n",
    "\n",
    "# Hugging Face & Transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "# LangChain for RAG logic\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# LangGraph for agentic execution\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Utilities\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c53e32-23c5-4191-bb4f-21e3ee612665",
   "metadata": {},
   "source": [
    "### 2. Loading and saving the data\n",
    "> Loading a legacy dataset from HuggingFace.\n",
    "\n",
    "> Saving 500 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c302efc8-ee8a-4665-9f88-e90c1cfedbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in streaming mode, because it takes up less disc space\n",
    "streamed_dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\", streaming=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86d2661b-9b76-4552-a847-d9e80151dfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 500 articles.\n"
     ]
    }
   ],
   "source": [
    "# Getting a sample from the data\n",
    "sampled_data = []\n",
    "for i, article in enumerate(streamed_dataset):\n",
    "    if i >= 500:\n",
    "        break\n",
    "    sampled_data.append(article)\n",
    "\n",
    "print(f\"Collected {len(sampled_data)} articles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d32e5d3c-c649-46d3-a31f-70f3f986e176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Anarchism\n",
      "2. Autism\n",
      "3. Albedo\n",
      "4. A\n",
      "5. Alabama\n",
      "6. Achilles\n",
      "7. Abraham Lincoln\n",
      "8. Aristotle\n",
      "9. An American in Paris\n",
      "10. Academy Award for Best Production Design\n"
     ]
    }
   ],
   "source": [
    "# Print titles of the first 10 articles\n",
    "for i, article in enumerate(sampled_data[:10]):\n",
    "    print(f\"{i+1}. {article['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9051baf-b30a-475c-9dc5-5798fba342e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Academy Award for Best Production Design\n",
      "URL: https://en.wikipedia.org/wiki/Academy%20Award%20for%20Best%20Production%20Design\n",
      "\n",
      "The Academy Award for Best Production Design recognizes achievement for art direction in film. The category's original name was Best Art Direction, but was changed to its current name in 2012 for the 85th Academy Awards. This change resulted from the Art Director's branch of the Academy of Motion Pi\n"
     ]
    }
   ],
   "source": [
    "# Viewing one article\n",
    "sample_article = sampled_data[0]\n",
    "\n",
    "print(f\"Title: {article['title']}\")\n",
    "print(f\"URL: {article['url']}\\n\")\n",
    "print(article['text'][:300])  # Show only the first 300 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af0aa49e-7665-4ad0-be0c-df420e704fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 articles to 'data'\n"
     ]
    }
   ],
   "source": [
    "# Saving articles\n",
    "\n",
    "# Creating a directory\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Save each article\n",
    "for i, article in enumerate(sampled_data):\n",
    "    filename = f\"{article['title']}.json\"\n",
    "    filepath = os.path.join(\"data\", filename)\n",
    "    \n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(article, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved {len(sampled_data)} articles to 'data'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5b38d-0783-4fe3-8bc3-b4c7f0d7b7e8",
   "metadata": {},
   "source": [
    "### 3. Splitting text into chunks\n",
    "> Converting articles to LangChain documents\n",
    "\n",
    "> Specifying chunk size and overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "face5b2e-e177-4748-a5bf-51b9792f64e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of text chunks: 46103\n",
      "\n",
      "Sample chunk (from article: Anarchism):\n",
      "\n",
      "Anarchism is a political philosophy and movement that is sceptical of authority and rejects all involuntary, coercive forms of hierarchy. Anarchism calls for the abolition of the state, which it holds to be unnecessary, undesirable, and harmful. As a historically left-wing movement, placed on the farthest left of the political spectrum, it is usually described alongside communalism and libertarian\n"
     ]
    }
   ],
   "source": [
    "# LangChain Document objects\n",
    "documents = [Document(page_content=article[\"text\"], metadata={\"title\": article[\"title\"]}) for article in sampled_data]\n",
    "\n",
    "# Use recursive character splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,      # Characters per chunk\n",
    "    chunk_overlap=50     # Overlap between chunks to preserve context\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total number of text chunks: {len(chunks)}\")\n",
    "print(f\"\\nSample chunk (from article: {chunks[0].metadata['title']}):\\n\")\n",
    "print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0929c52c-174e-4a9e-9c4f-ec70cff169a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average chunks per article: 92.21\n"
     ]
    }
   ],
   "source": [
    "# Average chunk number in one article\n",
    "print(f\"Average chunks per article: {len(chunks) / len(sampled_data):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051df625-8003-498a-a93e-e15468c5dc69",
   "metadata": {},
   "source": [
    "### 4. Creating embeddings and building vector database\n",
    "> Choosing an embedding model\n",
    "\n",
    "> Build vector database with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f2707c2-5dfd-4fee-8f38-ccbadc7e1e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99363/2206038898.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Load a sentence transformer model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "652487ae-3659-48b6-ab75-37e3614d281f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX512 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX512 support.\n",
      "INFO:faiss:Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vector store created and saved.\n"
     ]
    }
   ],
   "source": [
    "# Build FAISS vector store from document chunks\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "# Save it to disk\n",
    "vectorstore.save_local(\"vectorstore/faiss_index\")\n",
    "\n",
    "print(\"FAISS vector store created and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f64c985-fccd-465d-b6f5-afdee5e5fed9",
   "metadata": {},
   "source": [
    "### 5. Define tools and agent\n",
    "> Define search tool\n",
    "\n",
    "> Define agent with tool\n",
    "\n",
    "> Define nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "080eb118-8b3f-4895-8042-ac9bc9cbf6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining tool for searching\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"vectorstore_search\",\n",
    "    description=\"Searches the document store for relevant context.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5209cad-2ca8-47a2-a78c-ee02f986b82d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from accelerate>=0.26.0) (24.2)\n",
      "Requirement already satisfied: psutil in /home/codespace/.local/lib/python3.12/site-packages (from accelerate>=0.26.0) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/codespace/.local/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.6.0+cpu)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.31.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.6.1)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2025.1.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"accelerate>=0.26.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf73862-408c-4318-9070-7ba69efa077d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m model_id = \u001b[33m\"\u001b[39m\u001b[33mtiiuae/falcon-rw-1b\u001b[39m\u001b[33m\"\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#token = os.getenv(\"HUGGINGFACE_TOKEN\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m.from_pretrained(model_id)\n\u001b[32m      5\u001b[39m model = AutoModelForCausalLM.from_pretrained(model_id)\n\u001b[32m      7\u001b[39m pipe = pipeline(\u001b[33m\"\u001b[39m\u001b[33mtext-generation\u001b[39m\u001b[33m\"\u001b[39m, model=model, tokenizer=tokenizer)\n",
      "\u001b[31mNameError\u001b[39m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining generator model\n",
    "model_id = \"tiiuae/falcon-rw-1b\" \n",
    "#token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Wrap it for LangChain use\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d9a1f34-98f5-4f8b-bd7d-42230a418169",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f86383d-c76d-4743-9b1d-263731f34abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with tool\n",
    "\n",
    "# Define input/output state\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    output: str\n",
    "\n",
    "def agent_step(state: AgentState) -> AgentState:\n",
    "    query = state[\"input\"]\n",
    "    docs = retriever_tool.invoke(query)\n",
    "    context = \"\\n\\n\".join([doc.page_content if hasattr(doc, \"page_content\") else str(doc) for doc in docs])\n",
    "    prompt = f\"Use the following context to answer:\\n\\n{context}\\n\\nQuestion: {query}\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"input\": query, \"output\": response}\n",
    "\n",
    "# 4. Build LangGraph\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"agent\", agent_step)\n",
    "graph.set_entry_point(\"agent\")\n",
    "graph.add_edge(\"agent\", END)\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a6f26d4-0c65-4929-854a-6139dbb66f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieval-augmented generation\n"
     ]
    }
   ],
   "source": [
    "response = app.invoke({\"input\": \"What is retrieval-augmented generation?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4b1d97b-fdcd-4744-bea0-db1d3833d312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieval-augmented generation\n"
     ]
    }
   ],
   "source": [
    "respone = app.invoke({\"input\": \"Who was Achilles?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fe0da2-1994-4cf8-915e-c55adb129da3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
